{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T17:09:25.779596Z",
     "start_time": "2025-10-07T17:06:17.497120Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#collect_imgs\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "DATA_DIR = './data'\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "number_of_classes = 9\n",
    "dataset_size = 200\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "for j in range(number_of_classes):\n",
    "    if not os.path.exists(os.path.join(DATA_DIR, str(j))):\n",
    "        os.makedirs(os.path.join(DATA_DIR, str(j)))\n",
    "\n",
    "    print('Collecting data for class {}'.format(j))\n",
    "\n",
    "    done = False\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.putText(frame, 'Ready? Press \"Q\" ! :)', (100, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3,\n",
    "                    cv2.LINE_AA)\n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(25) == ord('q'):\n",
    "            break\n",
    "\n",
    "    counter = 0\n",
    "    while counter < dataset_size:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imshow('frame', frame)\n",
    "        cv2.waitKey(25)\n",
    "        cv2.imwrite(os.path.join(DATA_DIR, str(j), '{}.jpg'.format(counter)), frame)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f273f227a9913",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T17:10:56.797932Z",
     "start_time": "2025-10-07T17:09:36.014408Z"
    }
   },
   "outputs": [],
   "source": [
    "#create_dataset\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Set max_num_hands to 2\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.3)\n",
    "\n",
    "DATA_DIR = './data'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for dir_ in os.listdir(DATA_DIR):\n",
    "    if not os.path.isdir(os.path.join(DATA_DIR, dir_)):\n",
    "        continue\n",
    "\n",
    "    for img_path in os.listdir(os.path.join(DATA_DIR, dir_)):\n",
    "        if not img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            continue\n",
    "\n",
    "        data_aux = []\n",
    "        x_all = []\n",
    "        y_all = []\n",
    "\n",
    "        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))\n",
    "        if img is None:\n",
    "            continue  \n",
    "\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = hands.process(img_rgb)\n",
    "\n",
    "        if results.multi_hand_landmarks and len(results.multi_hand_landmarks) == 2:\n",
    "            results.multi_hand_landmarks = sorted(\n",
    "                results.multi_hand_landmarks,\n",
    "                key=lambda hand_landmarks: hand_landmarks.landmark[0].x\n",
    "            )\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "\n",
    "            for hand_landmarks in results.multi_hand_landmarks[:2]: \n",
    "                x_ = []\n",
    "                y_ = []\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                    x_.append(x)\n",
    "                    y_.append(y)\n",
    "\n",
    "                x_all.extend(x_)\n",
    "                y_all.extend(y_)\n",
    "\n",
    "                min_x = min(x_)\n",
    "                min_y = min(y_)\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    data_aux.append(x_[i] - min_x)\n",
    "                    data_aux.append(y_[i] - min_y)\n",
    "\n",
    "            expected_feature_size = 42 * 2\n",
    "            current_size = len(data_aux)\n",
    "\n",
    "            if current_size == 42:\n",
    "                data_aux.extend([0.0] * 42)\n",
    "            elif current_size == 0:\n",
    "                continue\n",
    "            elif current_size != expected_feature_size:\n",
    "                continue\n",
    "\n",
    "            data.append(data_aux)\n",
    "            labels.append(dir_)\n",
    "\n",
    "f = open('data.pickle', 'wb')\n",
    "pickle.dump({'data': data, 'labels': labels}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e259d5ba9c27fcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T17:11:10.973664Z",
     "start_time": "2025-10-07T17:11:09.063430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.41772151898735% of samples were classified correctly !\n"
     ]
    }
   ],
   "source": [
    "#train_classifier\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_dict = pickle.load(open('./data.pickle', 'rb'))\n",
    "\n",
    "data = np.asarray(data_dict['data'])\n",
    "labels = np.asarray(data_dict['labels'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "score = accuracy_score(y_predict, y_test)\n",
    "\n",
    "print('{}% of samples were classified correctly !'.format(score * 100))\n",
    "\n",
    "f = open('model.p', 'wb')\n",
    "pickle.dump({'model': model}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7573ce7462fccdc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T17:39:17.212222Z",
     "start_time": "2025-10-07T17:37:30.527970Z"
    }
   },
   "outputs": [],
   "source": [
    "#synapse_interactive.py\n",
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "import speech_recognition as sr\n",
    "\n",
    "print(\"Available microphones:\")\n",
    "for index, name in enumerate(sr.Microphone.list_microphone_names()):\n",
    "    print(f\"  Microphone \\\"{name}\\\" found for `Microphone(device_index={index})`\")\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.3)\n",
    "\n",
    "gesture_to_phrase = {\n",
    "    0: 'Hello',\n",
    "    1: 'Hello',\n",
    "    2: 'Good Job',\n",
    "    3: 'Good Job',\n",
    "    4: 'Namaste',\n",
    "    5: 'Home',\n",
    "    6: 'Peace',\n",
    "    7: 'Peace',\n",
    "    8: 'NO!',\n",
    "}\n",
    "\n",
    "phrase_to_gesture = {\n",
    "    \"hello\": \"hello.jpg\",\n",
    "    \"good job\": \"goodjob.jpeg\",\n",
    "    \"namaste\": \"namaste.jpeg\",\n",
    "    \"home\": \"home.jpg\",\n",
    "    \"peace\": \"Peace.jpeg\",\n",
    "    \"no\": \"no.jpg\"\n",
    "}\n",
    "\n",
    "def text_to_speech(text):\n",
    "    \"\"\" Converts text to speech and plays it. Caches audio files. \"\"\"\n",
    "    try:\n",
    "        filename = f\"./temp_{text.replace(' ', '_').replace('!', '')}.mp3\"\n",
    "\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"Generating audio for: '{text}'\")\n",
    "            tts = gTTS(text=text, lang='en')\n",
    "            tts.save(filename)\n",
    "\n",
    "        playsound(filename)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in text_to_speech: {e}\")\n",
    "\n",
    "def display_gesture_image(gesture_name):\n",
    "    \"\"\" Displays an image of the recognized gesture. \"\"\"\n",
    "    image_path = os.path.join('gesture_images', gesture_name)\n",
    "    if os.path.exists(image_path):\n",
    "        img = cv2.imread(image_path)\n",
    "        cv2.imshow('Spoken Gesture', img)\n",
    "        cv2.waitKey(2000) \n",
    "        cv2.destroyWindow('Spoken Gesture')\n",
    "    else:\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "\n",
    "def listen_for_commands():\n",
    "    \"\"\" Runs in a separate thread to listen for voice commands. \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "    microphone = sr.Microphone(device_index=1)\n",
    "\n",
    "    with microphone as source:\n",
    "        print(\"Calibrating for ambient noise, please be quiet...\")\n",
    "        recognizer.adjust_for_ambient_noise(source, duration=2)\n",
    "        print(\"Calibration complete. Listening for commands...\")\n",
    "\n",
    "    while True:\n",
    "        with microphone as source:\n",
    "            try:\n",
    "                audio = recognizer.listen(source, timeout=5, phrase_time_limit=4)\n",
    "                text = recognizer.recognize_google(audio).lower()\n",
    "                print(f\"You said: {text}\")\n",
    "                if text in phrase_to_gesture:\n",
    "                    gesture_image_name = phrase_to_gesture[text]\n",
    "                    print(f\"Recognized command! Displaying: {gesture_image_name}\")\n",
    "                    display_gesture_image(gesture_image_name)\n",
    "\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"DEBUG: Could not understand the audio. Please speak more clearly.\")\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Could not request results from Google API; {e}\")\n",
    "            except Exception as e:\n",
    "                pass \n",
    "        time.sleep(0.1) \n",
    "\n",
    "listener_thread = threading.Thread(target=listen_for_commands, daemon=True)\n",
    "listener_thread.start()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "last_gesture_spoken = None\n",
    "last_speech_time = time.time()\n",
    "speech_cooldown = 3 \n",
    "\n",
    "print(\"\\nWebcam and gesture recognition started. Look at the camera!\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        data_aux = []\n",
    "        x_all, y_all = [], []\n",
    "\n",
    "        if len(results.multi_hand_landmarks) == 2:\n",
    "            results.multi_hand_landmarks = sorted(\n",
    "                results.multi_hand_landmarks,\n",
    "                key=lambda hand_landmarks: hand_landmarks.landmark[0].x\n",
    "            )\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "            x_, y_ = [], []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                x_.append(landmark.x)\n",
    "                y_.append(landmark.y)\n",
    "\n",
    "            min_x, min_y = min(x_), min(y_)\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                data_aux.append(x_[i] - min_x)\n",
    "                data_aux.append(y_[i] - min_y)\n",
    "\n",
    "        if len(data_aux) == 42:\n",
    "            data_aux.extend([0.0] * 42)\n",
    "\n",
    "        if len(data_aux) == 84:\n",
    "            prediction = model.predict([np.asarray(data_aux)])\n",
    "            predicted_class = int(prediction[0])\n",
    "            predicted_character = gesture_to_phrase.get(predicted_class, 'Unknown')\n",
    "\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    x_all.append(landmark.x)\n",
    "                    y_all.append(landmark.y)\n",
    "\n",
    "            x1, y1 = int(min(x_all) * W) - 10, int(min(y_all) * H) - 10\n",
    "            x2, y2 = int(max(x_all) * W) + 10, int(max(y_all) * H) + 10\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "            cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "            current_time = time.time()\n",
    "            if predicted_class != last_gesture_spoken or (current_time - last_speech_time) > speech_cooldown:\n",
    "                tts_thread = threading.Thread(target=text_to_speech, args=(predicted_character,), daemon=True)\n",
    "                tts_thread.start()\n",
    "                last_gesture_spoken = predicted_class\n",
    "                last_speech_time = current_time\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
