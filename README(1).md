
# Interactive AI Gesture & Speech Recognition System


A real-time, interactive AI system that translates hand gestures into audible speech and spoken words into visual gestures, creating a seamless, two-way communication experience.


## ğŸŒŸ Features

Gesture-to-Speech: Performs a gesture and the system provides instant, synthesized speech feedback.

* **Gesture-to-Speech**: Performs a gesture (e.g., thumbs up ğŸ‘) and the system responds with a synthesized voice ("Great job!").
* **Speech-to-Gesture**: Speaks a command (e.g., "Hello") and the system displays a corresponding image of the gesture (e.g., a wave ğŸ‘‹).
* **Real-Time Recognition**: Low-latency processing for a natural and seamless interaction.
* **Multi-Hand Tracking**: Capable of recognizing gestures performed with one or two hands.
* **Extendable**: Easily customizable to add new gestures and voice commands.

## Tech Stack
* **Programming Language**: Python
* **Core Libraries**:
    * **Computer Vision**: OpenCV, MediaPipe
    * **Machine Learning**: scikit-learn
    * **Speech Synthesis (TTS)**: gTTS, playsound
    * **Speech Recognition (STT)**: SpeechRecognition, PyAudio
## ğŸš€ Getting Started
Follow these instructions to set up and run the project on your local machine.

### 1. Prerequisites

* Python 3.8+
* A webcam and a microphone

### 2. Installation & Setup

**a. Project Structure:**

Project Interactive-Gesture-Speech-System/
â”‚
â”œâ”€â”€ .venv/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ 0/
â”‚   â”‚   â”œâ”€â”€ 0.jpg
â”‚   â”‚   â”œâ”€â”€ 1.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ 1/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ (etc.)
â”‚
â”œâ”€â”€ gesture_images/
â”‚   â”œâ”€â”€ A.jpg
â”‚   â”œâ”€â”€ B.jpg
â”‚   â”œâ”€â”€ thumbs_up.jpg
â”‚   â””â”€â”€ (etc.)
â”‚
â”œâ”€â”€ collect_imgs.py
â”œâ”€â”€ create_dataset.py
â”œâ”€â”€ train_classifier.py
â”œâ”€â”€ synapse_interactive.py
â”‚
â”œâ”€â”€ model.p
â”œâ”€â”€ data.pickle
â”‚
â””â”€â”€ README.md



**b. Create and activate a virtual environment:**

```bash
# For Windows
python -m venv .venv
.venv\Scripts\activate
```
**c. Install the required libraries:**

**Core Libraries**

These libraries provide the main functionalities for computer vision, machine learning, and user interaction.

* **opencv-python:** The primary library for all computer vision tasks, including capturing webcam video and displaying images.

* **mediapipe:** Used for real-time hand tracking and landmark detection.

* **scikit-learn:** Used to train the RandomForestClassifier for gesture recognition.

* **numpy:** A fundamental library for numerical operations, used to handle the data arrays.

**Speech Functionality Libraries**

These libraries were added to handle the text-to-speech and speech-to-text features.

* **gTTS:** (Google Text-to-Speech) Used to convert text phrases into audible speech.

* **playsound:** A simple library used to play the audio files generated by gTTS.

* **SpeechRecognition:** The main library for capturing microphone audio and converting speech to text.

* **PyAudio:** Required by SpeechRecognition to access the microphone's audio stream.

### 2. Folder Structure

Make sure you create a folder named **gesture_images** and place one clear .jpg or .png file for each gesture you want to be displayed.
## Usage
The project is divided into three main steps: data collection, model training, and running the interactive application:

**1. Collect Gesture Data:** Run the **collect_imgs.py** script to capture images for your gestures. You will be prompted to **press 'q'** to start capturing for each class.

```Bash
python collect_imgs.py
```
**2. Create the Dataset:** Run the **create_dataset.py** script. This will process all the images in the data folder and create a data.pickle file.

```Bash
python create_dataset.py
```
**3. Train the Model:** Run the **train_classifier.py** script. This will use **data.pickle** to train the gesture recognition model and save it as **model.p**

```Bash
python train_classifier.py
```
**4.Run the Interactive Application:** You are now ready to run the main application!

```Bash
python synapse_interactive.py
```
Look at your webcam, perform gestures to hear the speech output, and speak commands to see the gesture images appear. Press 'q' to quit.



